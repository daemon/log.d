<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>phone-error-analysis</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="/log.d/pandoc.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#phonetic-error-analysis">Phonetic Error Analysis</a>
<ul>
<li><a href="#research-questions">Research Questions</a></li>
<li><a href="#methods">Methods</a></li>
</ul></li>
</ul>
</nav>
<h1 id="phonetic-error-analysis">Phonetic Error Analysis</h1>
<h2 id="research-questions">Research Questions</h2>
<ol type="1">
<li>Given the occurrence of a speech recognition error, is it more likely that the error happens between similar sounding but distinct transcriptions? (Is phonetic edit distance most standard? Literature review of metrics?)</li>
<li>For nonstandard pronunciations (e.g., those that deviate from “standard” American English), are certain phonemic changes more predictive of lower speech recognition system quality? If so, which ones? (L1, L2 language problem. System that transcribes audio to vowel coordinates?)</li>
<li>What is the relationship between the phonetic edit distance, the lexical edit distance, the number of training examples for each class, and the likelihood of making a speech recognition error?</li>
</ol>
<h2 id="methods">Methods</h2>
<p>Let there be a sequence of audio-transcription pairs <span class="math inline">\((x_{i_n}, y_{i_n}), x_{i_n} \in \mathcal{X}, y_{i_n} \in \mathcal{Y}\)</span> indexed by <span class="math inline">\(i_n \in \mathcal{T} = \mathcal{T}_\text{training} \cup \mathcal{T}_\text{dev} \cup \mathcal{T}_\text{test}\)</span>, where <span class="math inline">\(y_i\)</span> is the orthographic string represented by the speech <span class="math inline">\(x_i\)</span>. Let a parametric classifier <span class="math inline">\(f(x; \theta)\)</span> produce a probability distribution over <span class="math inline">\(\mathcal{Y}\)</span> given audio <span class="math inline">\(x\)</span>. Let <span class="math inline">\(\phi(x_i, y_i)\)</span> return the phonetic transcription of the audio-transcription tuple; for the sake of simplicity, assume that <span class="math inline">\(\forall i, j \in \mathcal{T}.y_i \neq y_j \implies \phi(x_i, y_i) \neq \phi(x_j, y_j)\)</span>, which holds for limited speech recognition. Let <span class="math inline">\(\boldsymbol D&#39;\)</span> be the distance matrix induced by the edit distance string metric <span class="math inline">\(\delta(s_1, s_2)\)</span> among all distinct pairs of phonetic transcriptions <span class="math inline">\(\mathcal{Z}\)</span>, ordered lexicographically by the index set <span class="math inline">\(\mathcal{I}\)</span> along the rows and columns. Let <span class="math inline">\(\boldsymbol C\)</span> be the confusion matrix from evaluating a speech classification model on the test set, with the rows matching those of <span class="math inline">\(\boldsymbol D&#39;\)</span> and columns <span class="math inline">\(\mathcal{J}\)</span> being the index set of orthographic predictions <span class="math inline">\(\mathcal{Y}\)</span>, and <span class="math inline">\(\boldsymbol C_{ij}\)</span> is the number of times the model <em>erroneously</em> predicted the <span class="math inline">\(i^\text{th}\)</span> true phonetic transcription as the <span class="math inline">\(j^\text{th}\)</span> orthographic transcription.</p>
<p><strong>RQ1: Phonetic confusion correlation coefficient (PC3).</strong> The expected Pearson’s correlation coeff. between the phonetic edit distance and the number of confusions, with the confusions drawn from some distribution (e.g., the traffic): <span class="math display">\[
\rho = \mathbb{E}_Z[r(U, V|Z)],
\]</span> where <span class="math inline">\(Z\)</span> is the phonetic transcription distributed according to the traffic, <span class="math inline">\(U\)</span> is the phonetic edit distance between <span class="math inline">\(Z\)</span> and the model prediction, <span class="math inline">\(r\)</span> is the correlation function (Pearson’s) with the r.v.’s conditioned on <span class="math inline">\(Z\)</span>, and <span class="math inline">\(V\)</span> is the number of erroneous confusions for <span class="math inline">\(Z\)</span>. Let <span class="math inline">\(\Phi(y)\)</span> produce the indices in <span class="math inline">\(\mathcal{I}\)</span> of the possible phonetic transcriptions for the string <span class="math inline">\(y\)</span>. Since the phonetic error <span class="math inline">\(U\)</span> of the model is unavailable (it outputs orthographic transcriptions), we take it as a latent variable that depends on <span class="math inline">\(f(x; \theta)\)</span>. For simplicity, we assume that this latent variable is the minimum possible phonetic distance between <span class="math inline">\(Z\)</span> and the set of candidates <span class="math inline">\(\Phi(f(x; \theta))\)</span>, i.e., the model has no intraclass phonetic bias, just interclass phonetic bias. Let <span class="math inline">\(\boldsymbol D_{ij}\)</span> be this minimum possible phonetic edit distance between <span class="math inline">\(z_i, i \in \mathcal{I}\)</span> and <span class="math inline">\(\Phi(y): z_i \in \Phi(y)\)</span>. Our estimator is henceforth computed as <span class="math display">\[
\hat{\rho} = \sum_{i \in \mathcal{J}} w_i r_i,\hspace{5mm} w_i = \frac{\sum_{\boldsymbol C_{ij} \neq 0} \boldsymbol C_{ij}}{\sum_{\boldsymbol C_{kj}\neq0} \boldsymbol C_{kj}},\hspace{5mm} r_i = \frac{\sum_{\boldsymbol C_{ij} \neq 0}(\boldsymbol D_{ij} - \bar{\boldsymbol D_i}) (\boldsymbol C_{ij} - \bar{\boldsymbol C_i})}{\sqrt{\sum_{\boldsymbol C_{ij} \neq 0}(\boldsymbol D_{ij} - \bar{\boldsymbol D_i})^2}\sqrt{\sum_{\boldsymbol C_{ij} \neq 0}(\boldsymbol C_{ij} - \bar{\boldsymbol C_i})^2}}.
\]</span> By construction, <span class="math inline">\(\hat{\rho} \in [-1, 1]\)</span>, with positive values indicating a preference to make more errors for dissimilar transcripts and negative values indicating more for similar transcripts. To compute confidence intervals, use bootstrapping for now, but we can likely derive from normality assumptions and/or exact confidence distributions. If sampling according to the traffic distribution doesn’t matter (i.e., uniform at random), this is as simple as using Fisher’s <span class="math inline">\(r\)</span>-to-<span class="math inline">\(z\)</span> transformation for each <span class="math inline">\(r\)</span>.</p>
<p><strong>RQ2: Nonstandard Pronunciations.</strong> Define a phonetic perturbation function <span class="math inline">\(\Delta: \mathcal{Z} \mapsto 2^\mathcal{Z}\)</span> that satisfies the following properties:</p>
<ol type="1">
<li><span class="math inline">\(z \in \Delta(z)\)</span>. (Identity)</li>
<li>All strings with a deletion distance of 1 from <span class="math inline">\(z\)</span> is in <span class="math inline">\(\Delta(z)\)</span>.</li>
<li>All strings with a single consonant-consonant or vowel-vowel change from <span class="math inline">\(z\)</span> is in <span class="math inline">\(\Delta(z)\)</span>, e.g., <code>RATATA -&gt; RUTUTU</code> but not <code>CAT -&gt; AAT</code>. (consider <span class="math inline">\(\pm\)</span>continuous)</li>
<li>All other strings are not in <span class="math inline">\(\Delta(z)\)</span>.</li>
</ol>
<p>As an example, <span class="math inline">\(\Delta(\texttt{CAT}) = \{\texttt{MAT}, \texttt{DAT}, \texttt{COT}, \texttt{CA}, \dots\}\)</span>. Then, for each audio-transcription pair <span class="math inline">\((x, y)\)</span>, use forced alignment to find the most likely phoneme sequence in <span class="math inline">\(\bigcup \{\Delta(z): z \in\Phi(y)\}\)</span> of <span class="math inline">\(x\)</span>.</p>
</body>
</html>
